#import "template.typ": *

#show: project.with(
  title: "Assignment 2: Limit Order Book",
  authors: (
    (name: "Lee Kai Yang\n(23205838 - 100%)", email: "kai.y.lee@ucdconnect.ie"),
  ),
  date: "March 4, 2024",
  font: "CMU Serif",
  monofont: "CMU Typewriter Text",
)

#align(center)[
  #text(fill: blue, size: 9pt)[#underline(link("")[ðŸ”— Video Link])]
  #h(0.2em)
  #text(fill: blue, size: 9pt)[#underline(link("https://github.com/marcustut/datastructure/tree/main/lob")[ðŸ”— GitHub Link])]
]

#pad(x: 12pt, [
  *Abstract* - This project ...
])

= Background

Since the first development of a stock exchange back in the 17th century, open outcry or also known as pit trading has always been the main medium of communication between different parties. It involves shouting and the use of hand signals to transfer information on buy or sell orders. At the time, clerks were hired to bookkeep these orders typically written down in a book, hence the name "order book". However, this method is inefficient and highly error prone thus it was slowly replaced since 1970s by electronic trading systems with the NASDAQ becoming world's first electronic stock market in 1971. The very "heartbeat" of these electronic trading systems is the order book, every single buy or sell order has to make its way through the order book before it can be executed. Therefore, to build an efficient electronic trading system the order book must be capable of processing an enourmous amount of order requests in a short amount of time. In other words, high throughput and low latency. 

= Introduction

As its name suggest, an order book is a data structure that holds a collection of buy orders and sell orders. @orderbook_sketch below shows an illustration of a limit order book.

#figure(
  image("images/orderbook.png", width: 60%),
  caption: [Limit Order Book Illustration]
) <orderbook_sketch>

As can be seen, both sell orders (asks) and buy orders (bids) are grouped into levels in the book and these levels are called "limit". Within a limit there can be one or more orders and the sequence is usually based on the timestamp at which these orders come in to the system as most matching engines match the oldest order first. Note that each limit has a `price` and a `volume`, all orders at this limit has the same price and the volume is the aggregated quantity of all the orders. Moreover, the first limit is often referred to as top of book or best bid / best ask.

= Design decisions

Because of the high throughput and low latency requirement of limit order books, it is important to take into consideration the time complexities for important operations (which will be called repeatedly). The main operations for an order book is listed as follows:

- *Limit* - Adding a new order onto the book.
- *Cancel* - Cancel an existing order from the book.
- *Market* - Matching orders from the book (removing existing orders)
- *Amend* - Updating the quantity of a particular order.
- *Get Best Bid/Ask* - Get the current best bid or best ask.
- *Get Volume* - Get the current volume.

Ideally these operations should be

= Code design

...

= Performance benchmark

To measure how well our implementation of queue performs, we designed a benchmark to compare the latency and throughput of `mq.Queue` against queue implementations from the Java standard library, namely `LinkedList`, `PriorityQueue`, and `ArrayDeque`. The benchmark code can be found in `src/benchmark/Main.java` and these plots are generated by running `plot.py` which uses `matplotlib` to visualize the outputs from the benchmark program.

== Latency analysis

From above we can see two charts, one for the *offer* operation which enqueues an element at the tail, and the other for the *poll* operation which dequeues an element from the head. We can see that for the *offer* operation, both `ArrayDeque` and `PriorityQueue` performed significantly faster than `LinkedList` and `mq.Queue` which are both based on the linked list under the hood. We speculate that as many operations increase, the effect of cache locality starts to show its significance since both `ArrayDeque` and `PriorityQueue` use an array under the hood to store the elements. In addition, one interesting observation is that the latency for `ArrayDeque` spiked at around 0 - 10k operations, this is expected since the underlying array has to resize when the insertion exceeds the current capacity.

As for the *poll* operation, `PriorityQueue` performed the worst and from the graph, we can clearly see that `mq.Queue`, `ArrayDeque` and `LinkedList` scaled well as the number of operations increases which is close to constant time _O(1)_. This is because `PriorityQueue` uses a binary heap under the hood which implies ordering hence to poll (remove) an element it has to find the element in a heap which results in a _O(log n)_ time complexity.

== Throughput analysis 

above shows the throughput for the `offer` operation across different numbers of operations which measure the operations that the queue can process per second. Here we can observe that `ArrayDeque` has the highest throughput in most scenarios except at 10k operations where it performed marginally slower than the others, this is due to the resizing issue as can also be seen in above. As for `mq.Queue`, it performed generally well between 1k - 100k operations beating `LinkedList` and `PriorityQueue` and is on par with `ArrayDeque` at 1k operations.

above shows the throughput for the poll operation and as can be seen, both `mq.Queue` and `ArrayDeque` outperformed the others significantly with `mq.Queue` having higher throughput at 100 and 1000 operations and `ArrayDeque` performed the best in other number of operations.

== Conclusion

From the analysis that we have performed, we can deduce that generally `ArrayDeque` has the highest performance except for the fact that it has a resizing issue which can introduce latency spikes (jitter). Although this can be solved by reserving a large chunk of memory it incurs an overhead in memory cost when these reserved spaces are not being used. Considering that we do not expect high traffic for our message queue, `mq.Queue` is still the best choice for expected traffic of 0 - 100k operations while only using the memory that it needs.

#bibliography("references.yaml", style: "ieee")